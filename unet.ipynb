{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# UNet Model Testing Notebook\n",
       "\n",
       "This notebook tests the UNet model for urban crop yield prediction. The UNet architecture is used for image-to-image prediction, converting environmental data into crop yield predictions."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Imports"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import numpy as np\n",
       "import torch\n",
       "from torch.utils.data import DataLoader, random_split\n",
       "import matplotlib.pyplot as plt\n",
       "import sys\n",
       "\n",
       "# Add src to path for imports\n",
       "sys.path.append('.')\n",
       "\n",
       "# Import custom modules\n",
       "from src.models.unet import UNet\n",
       "from src.data.dataset import UrbanCropDataset\n",
       "from src.training import ModelTrainer\n",
       "from src.utils import set_seed, get_device, ensure_dir, visualize_prediction, calculate_metrics\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "set_seed(42)\n",
       "\n",
       "# Check for CUDA\n",
       "device = get_device()\n",
       "print(f\"Using device: {device}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Configuration"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configuration settings\n",
       "config = {\n",
       "    'data_dir': 'data/processed',  # Directory with processed data\n",
       "    'model_dir': 'models',         # Directory to save trained models\n",
       "    'visualize_dir': 'visualizations',  # Directory for visualizations\n",
       "    'batch_size': 16,              # Batch size for training\n",
       "    'epochs': 10,                  # Number of epochs (reduced for testing)\n",
       "    'learning_rate': 0.001,        # Learning rate\n",
       "    'test_split': 0.2,             # Fraction of data for testing\n",
       "    'val_split': 0.1               # Fraction of training data for validation\n",
       "}\n",
       "\n",
       "# Create necessary directories\n",
       "ensure_dir(config['model_dir'])\n",
       "ensure_dir(config['visualize_dir'])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Load Data\n",
       "\n",
       "For this notebook, we assume that the data has already been processed and saved as numpy arrays. If not, you'll need to run the data processing scripts first."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load processed data\n",
       "def load_data(config):\n",
       "    \"\"\"Load preprocessed data\"\"\"\n",
       "    x_path = os.path.join(config['data_dir'], 'years_array_32_segmented_prevUrb.npy')\n",
       "    y_path = os.path.join(config['data_dir'], 'crops_array_32_segmented_prevUrb.npy')\n",
       "    \n",
       "    if os.path.exists(x_path) and os.path.exists(y_path):\n",
       "        print(\"Loading preprocessed data...\")\n",
       "        X_data = np.load(x_path)\n",
       "        y_data = np.load(y_path)\n",
       "        print(f\"X_data shape: {X_data.shape}\")\n",
       "        print(f\"y_data shape: {y_data.shape}\")\n",
       "        return X_data, y_data\n",
       "    else:\n",
       "        raise FileNotFoundError(f\"Preprocessed data not found at {x_path} and {y_path}. Please run data preprocessing first.\")\n",
       "\n",
       "try:\n",
       "    X_data, y_data = load_data(config)\n",
       "    \n",
       "    # Sample visualization of the data\n",
       "    plt.figure(figsize=(15, 5))\n",
       "    \n",
       "    # Input features (first sample, first 3 channels)\n",
       "    plt.subplot(1, 2, 1)\n",
       "    plt.imshow(np.transpose(X_data[0][:3], (1, 2, 0)))\n",
       "    plt.title('Input Features (First 3 Channels)')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    # Target output\n",
       "    plt.subplot(1, 2, 2)\n",
       "    plt.imshow(np.transpose(y_data[0], (1, 2, 0)))\n",
       "    plt.title('Target Output')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "    \n",
       "except FileNotFoundError as e:\n",
       "    print(f\"Error: {e}\")\n",
       "    print(\"Using dummy data for demonstration purposes...\")\n",
       "    # Create dummy data for demonstration\n",
       "    X_data = np.random.rand(100, 15, 32, 32)  # [samples, channels, height, width]\n",
       "    y_data = np.random.rand(100, 3, 32, 32)   # [samples, channels, height, width]\n",
       "    print(f\"Dummy X_data shape: {X_data.shape}\")\n",
       "    print(f\"Dummy y_data shape: {y_data.shape}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Create Data Loaders\n",
       "\n",
       "Split the data into training, validation, and test sets, and create data loaders."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def create_data_loaders(X_data, y_data, config):\n",
       "    \"\"\"Create train, validation, and test data loaders\"\"\"\n",
       "    # Create dataset\n",
       "    dataset = UrbanCropDataset(X_data, y_data)\n",
       "    \n",
       "    # Determine the number of samples for each split\n",
       "    total_samples = len(dataset)\n",
       "    test_size = int(total_samples * config['test_split'])\n",
       "    train_size = total_samples - test_size\n",
       "    val_size = int(train_size * config['val_split'])\n",
       "    train_size = train_size - val_size\n",
       "    \n",
       "    print(f\"Total samples: {total_samples}\")\n",
       "    print(f\"Training samples: {train_size}\")\n",
       "    print(f\"Validation samples: {val_size}\")\n",
       "    print(f\"Test samples: {test_size}\")\n",
       "    \n",
       "    # Split into train, validation, and test sets\n",
       "    train_dataset, test_dataset = random_split(\n",
       "        dataset, [train_size + val_size, test_size],\n",
       "        generator=torch.Generator().manual_seed(42)\n",
       "    )\n",
       "    \n",
       "    train_dataset, val_dataset = random_split(\n",
       "        train_dataset, [train_size, val_size],\n",
       "        generator=torch.Generator().manual_seed(42)\n",
       "    )\n",
       "    \n",
       "    # Create data loaders\n",
       "    train_loader = DataLoader(\n",
       "        train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2\n",
       "    )\n",
       "    \n",
       "    val_loader = DataLoader(\n",
       "        val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2\n",
       "    )\n",
       "    \n",
       "    test_loader = DataLoader(\n",
       "        test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2\n",
       "    )\n",
       "    \n",
       "    return train_loader, val_loader, test_loader\n",
       "\n",
       "# Create data loaders\n",
       "train_loader, val_loader, test_loader = create_data_loaders(X_data, y_data, config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Initialize UNet Model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize UNet model\n",
       "def initialize_model(config):\n",
       "    \"\"\"Initialize UNet model and trainer\"\"\"\n",
       "    # Create UNet model\n",
       "    model = UNet(in_channels=15, out_channels=3)\n",
       "    print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
       "    \n",
       "    # Create trainer\n",
       "    trainer = ModelTrainer(model, device=device)\n",
       "    trainer.compile(\n",
       "        optimizer='adam',\n",
       "        learning_rate=config['learning_rate'],\n",
       "        criterion='mse'\n",
       "    )\n",
       "    \n",
       "    return model, trainer\n",
       "\n",
       "# Initialize model and trainer\n",
       "model, trainer = initialize_model(config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Train the Model\n",
       "\n",
       "Train the UNet model for a few epochs. For a full training, you would increase the number of epochs."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def train_model(trainer, train_loader, val_loader, config):\n",
       "    \"\"\"Train the model and save checkpoints\"\"\"\n",
       "    # Create callback for model checkpoint\n",
       "    from src.training import model_checkpoint, early_stopping\n",
       "    \n",
       "    checkpoint_path = os.path.join(config['model_dir'], 'unet_best.pth')\n",
       "    callbacks = [\n",
       "        model_checkpoint(checkpoint_path, monitor='val_loss', save_best_only=True),\n",
       "        early_stopping(patience=5, monitor='val_loss')\n",
       "    ]\n",
       "    \n",
       "    # Train the model\n",
       "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
       "    try:\n",
       "        history = trainer.fit(\n",
       "            train_loader,\n",
       "            val_loader=val_loader,\n",
       "            epochs=config['epochs'],\n",
       "            callbacks=callbacks\n",
       "        )\n",
       "    except KeyboardInterrupt:\n",
       "        print(\"Training interrupted by user\")\n",
       "    \n",
       "    # Save the final model\n",
       "    model_path = os.path.join(config['model_dir'], 'unet_final.pth')\n",
       "    trainer.save_model(model_path)\n",
       "    \n",
       "    return trainer\n",
       "\n",
       "# Train the model (uncomment to run training)\n",
       "# trainer = train_model(trainer, train_loader, val_loader, config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Plot Training History\n",
       "\n",
       "After training, plot the loss and metrics over epochs."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot training history (run after training)\n",
       "def plot_history(trainer, config):\n",
       "    \"\"\"Plot training history\"\"\"\n",
       "    history_path = os.path.join(config['visualize_dir'], 'unet_history.png')\n",
       "    trainer.plot_history(save_path=history_path)\n",
       "    \n",
       "# If you've trained the model, uncomment to plot the history\n",
       "# plot_history(trainer, config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Load Best Model and Evaluate\n",
       "\n",
       "Load the best model from checkpoint and evaluate on the test set."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def load_best_model(config):\n",
       "    \"\"\"Load the best model from checkpoint\"\"\"\n",
       "    # Initialize model\n",
       "    model = UNet(in_channels=15, out_channels=3)\n",
       "    trainer = ModelTrainer(model, device=device)\n",
       "    trainer.compile(optimizer='adam', learning_rate=config['learning_rate'], criterion='mse')\n",
       "    \n",
       "    # Load checkpoint\n",
       "    checkpoint_path = os.path.join(config['model_dir'], 'unet_best.pth')\n",
       "    if os.path.exists(checkpoint_path):\n",
       "        print(f\"Loading model from {checkpoint_path}\")\n",
       "        trainer.load_model(checkpoint_path)\n",
       "    else:\n",
       "        print(f\"Checkpoint not found at {checkpoint_path}. Using untrained model.\")\n",
       "    \n",
       "    return model, trainer\n",
       "\n",
       "# Load the best model\n",
       "# Uncomment after training or if you have a saved model\n",
       "# best_model, best_trainer = load_best_model(config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Evaluate on Test Set"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def evaluate_model(trainer, test_loader, config):\n",
       "    \"\"\"Evaluate the model on the test set\"\"\"\n",
       "    print(\"Evaluating model on test set...\")\n",
       "    test_loss, test_metrics = trainer.evaluate(test_loader)\n",
       "    print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_metrics['mae']:.4f}\")\n",
       "    \n",
       "    # Visualize some predictions\n",
       "    vis_path = os.path.join(config['visualize_dir'], 'unet_predictions.png')\n",
       "    visualize_prediction(trainer.model, test_loader, num_samples=5, save_path=vis_path)\n",
       "    \n",
       "    return test_loss, test_metrics\n",
       "\n",
       "# Evaluate on test set\n",
       "# Uncomment after loading a trained model\n",
       "# test_loss, test_metrics = evaluate_model(best_trainer, test_loader, config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 10. Make Predictions on New Data\n",
       "\n",
       "Use the trained model to make predictions on new data samples."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def predict_on_sample(model, X_data, idx=0):\n",
       "    \"\"\"Make a prediction on a single sample\"\"\"\n",
       "    model.eval()\n",
       "    model.to(device)\n",
       "    \n",
       "    # Get a sample\n",
       "    x = torch.tensor(X_data[idx:idx+1], dtype=torch.float32).to(device)\n",
       "    \n",
       "    # Make prediction\n",
       "    with torch.no_grad():\n",
       "        pred = model(x)\n",
       "    \n",
       "    # Convert to numpy\n",
       "    pred = pred.cpu().numpy()\n",
       "    \n",
       "    # Visualize\n",
       "    plt.figure(figsize=(10, 5))\n",
       "    \n",
       "    # Input (first 3 channels)\n",
       "    plt.subplot(1, 2, 1)\n",
       "    plt.imshow(np.transpose(X_data[idx][:3], (1, 2, 0)))\n",
       "    plt.title('Input (First 3 Channels)')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    # Prediction\n",
       "    plt.subplot(1, 2, 2)\n",
       "    plt.imshow(np.transpose(pred[0], (1, 2, 0)))\n",
       "    plt.title('Model Prediction')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "    \n",
       "    return pred\n",
       "\n",
       "# Make a prediction on a sample\n",
       "# Uncomment after loading a trained model\n",
       "# sample_idx = 10  # Change to try different samples\n",
       "# prediction = predict_on_sample(best_model, X_data, idx=sample_idx)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 11. Conclusion\n",
       "\n",
       "This notebook demonstrated how to load, train, evaluate, and use the UNet model for urban crop yield prediction. For a full training, you would increase the number of epochs and potentially tune hyperparameters for better performance."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }