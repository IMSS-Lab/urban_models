{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Bidirectional LSTM-UNet Model Testing Notebook\n",
       "\n",
       "This notebook tests the Bidirectional LSTM-UNet model for urban crop yield prediction. This model enhances the basic LSTM-UNet by using bidirectional LSTMs for improved temporal information processing."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Imports"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import os\n",
       "import numpy as np\n",
       "import torch\n",
       "from torch.utils.data import DataLoader, random_split\n",
       "import matplotlib.pyplot as plt\n",
       "import sys\n",
       "\n",
       "# Add src to path for imports\n",
       "sys.path.append('.')\n",
       "\n",
       "# Import custom modules\n",
       "from src.models.lstm_unet import BidirectionalLSTMUNet\n",
       "from src.data.dataset import TemporalUrbanCropDataset\n",
       "from src.training import LSTMUNetTrainer\n",
       "from src.utils import set_seed, get_device, ensure_dir, visualize_prediction, calculate_metrics\n",
       "\n",
       "# Set random seed for reproducibility\n",
       "set_seed(42)\n",
       "\n",
       "# Check for CUDA\n",
       "device = get_device()\n",
       "print(f\"Using device: {device}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Configuration"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configuration settings\n",
       "config = {\n",
       "    'data_dir': 'data/processed',  # Directory with processed data\n",
       "    'model_dir': 'models',         # Directory to save trained models\n",
       "    'visualize_dir': 'visualizations',  # Directory for visualizations\n",
       "    'batch_size': 16,              # Batch size for training\n",
       "    'epochs': 10,                  # Number of epochs (reduced for testing)\n",
       "    'learning_rate': 0.001,        # Learning rate\n",
       "    'test_split': 0.2,             # Fraction of data for testing\n",
       "    'val_split': 0.1,              # Fraction of training data for validation\n",
       "    'time_steps': 23               # Number of time steps for LSTM processing\n",
       "}\n",
       "\n",
       "# Create necessary directories\n",
       "ensure_dir(config['model_dir'])\n",
       "ensure_dir(config['visualize_dir'])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Load Data\n",
       "\n",
       "For this notebook, we assume that the data has already been processed and saved as numpy arrays. If not, you'll need to run the data processing scripts first."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load processed data\n",
       "def load_data(config):\n",
       "    \"\"\"Load preprocessed data\"\"\"\n",
       "    x_path = os.path.join(config['data_dir'], 'years_array_32_segmented_prevUrb.npy')\n",
       "    y_path = os.path.join(config['data_dir'], 'crops_array_32_segmented_prevUrb.npy')\n",
       "    \n",
       "    if os.path.exists(x_path) and os.path.exists(y_path):\n",
       "        print(\"Loading preprocessed data...\")\n",
       "        X_data = np.load(x_path)\n",
       "        y_data = np.load(y_path)\n",
       "        print(f\"X_data shape: {X_data.shape}\")\n",
       "        print(f\"y_data shape: {y_data.shape}\")\n",
       "        return X_data, y_data\n",
       "    else:\n",
       "        raise FileNotFoundError(f\"Preprocessed data not found at {x_path} and {y_path}. Please run data preprocessing first.\")\n",
       "\n",
       "try:\n",
       "    X_data, y_data = load_data(config)\n",
       "    \n",
       "    # Sample visualization of the data\n",
       "    plt.figure(figsize=(15, 5))\n",
       "    \n",
       "    # Input features (first sample, first 3 channels)\n",
       "    plt.subplot(1, 2, 1)\n",
       "    plt.imshow(np.transpose(X_data[0][:3], (1, 2, 0)))\n",
       "    plt.title('Input Features (First 3 Channels)')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    # Target output\n",
       "    plt.subplot(1, 2, 2)\n",
       "    plt.imshow(np.transpose(y_data[0], (1, 2, 0)))\n",
       "    plt.title('Target Output')\n",
       "    plt.axis('off')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.show()\n",
       "    \n",
       "except FileNotFoundError as e:\n",
       "    print(f\"Error: {e}\")\n",
       "    print(\"Using dummy data for demonstration purposes...\")\n",
       "    # Create dummy data for demonstration\n",
       "    # Note: For Bidirectional LSTM-UNet, we'll reshape this later\n",
       "    X_data = np.random.rand(100, 15, 32, 32)  # [samples, channels, height, width]\n",
       "    y_data = np.random.rand(100, 3, 32, 32)   # [samples, channels, height, width]\n",
       "    print(f\"Dummy X_data shape: {X_data.shape}\")\n",
       "    print(f\"Dummy y_data shape: {y_data.shape}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Create Data Loaders with Temporal Reshaping\n",
       "\n",
       "For Bidirectional LSTM-UNet, we need to reshape the data to include temporal information. We'll use the TemporalUrbanCropDataset class which handles this reshaping."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def create_data_loaders(X_data, y_data, config):\n",
       "    \"\"\"Create train, validation, and test data loaders with temporal reshaping\"\"\"\n",
       "    # Create dataset with temporal reshaping\n",
       "    dataset = TemporalUrbanCropDataset(X_data, y_data, time_steps=config['time_steps'])\n",
       "    \n",
       "    # Determine the number of samples for each split\n",
       "    total_samples = len(dataset)\n",
       "    test_size = int(total_samples * config['test_split'])\n",
       "    train_size = total_samples - test_size\n",
       "    val_size = int(train_size * config['val_split'])\n",
       "    train_size = train_size - val_size\n",
       "    \n",
       "    print(f\"Total samples: {total_samples}\")\n",
       "    print(f\"Training samples: {train_size}\")\n",
       "    print(f\"Validation samples: {val_size}\")\n",
       "    print(f\"Test samples: {test_size}\")\n",
       "    \n",
       "    # Split into train, validation, and test sets\n",
       "    train_dataset, test_dataset = random_split(\n",
       "        dataset, [train_size + val_size, test_size],\n",
       "        generator=torch.Generator().manual_seed(42)\n",
       "    )\n",
       "    \n",
       "    train_dataset, val_dataset = random_split(\n",
       "        train_dataset, [train_size, val_size],\n",
       "        generator=torch.Generator().manual_seed(42)\n",
       "    )\n",
       "    \n",
       "    # Create data loaders\n",
       "    train_loader = DataLoader(\n",
       "        train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2\n",
       "    )\n",
       "    \n",
       "    val_loader = DataLoader(\n",
       "        val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2\n",
       "    )\n",
       "    \n",
       "    test_loader = DataLoader(\n",
       "        test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2\n",
       "    )\n",
       "    \n",
       "    return train_loader, val_loader, test_loader\n",
       "\n",
       "# Create data loaders\n",
       "train_loader, val_loader, test_loader = create_data_loaders(X_data, y_data, config)\n",
       "\n",
       "# Check the shape of the data after temporal reshaping\n",
       "for x_batch, y_batch in train_loader:\n",
       "    print(f\"Batch shapes after temporal reshaping:\")\n",
       "    print(f\"x_batch shape: {x_batch.shape}\")  # Should be [batch, time_steps, height, width, channels_per_step]\n",
       "    print(f\"y_batch shape: {y_batch.shape}\")  # Should be [batch, channels, height, width]\n",
       "    break"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Initialize Bidirectional LSTM-UNet Model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize Bidirectional LSTM-UNet model\n",
       "def initialize_model(config):\n",
       "    \"\"\"Initialize Bidirectional LSTM-UNet model and trainer\"\"\"\n",
       "    # Create Bidirectional LSTM-UNet model\n",
       "    # input_shape=(time_steps, height, width, channels_per_step)\n",
       "    channels_per_step = 15 // config['time_steps']\n",
       "    model = BidirectionalLSTMUNet(input_shape=(config['time_steps'], 32, 32, channels_per_step), \n",
       "                                lstm_units=16, unet_filters=16)\n",
       "    \n",
       "    print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
       "    \n",
       "    # Create trainer\n",
       "    trainer = LSTMUNetTrainer(model, device=device)\n",
       "    trainer.compile(\n",
       "        optimizer='adam',\n",
       "        learning_rate=config['learning_rate'],\n",
       "        criterion='mse'\n",
       "    )\n",
       "    \n",
       "    return model, trainer\n",
       "\n",
       "# Initialize model and trainer\n",
       "model, trainer = initialize_model(config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Train the Model\n",
       "\n",
       "Train the Bidirectional LSTM-UNet model for a few epochs. For full training, you would increase the number of epochs."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def train_model(trainer, train_loader, val_loader, config):\n",
       "    \"\"\"Train the model and save checkpoints\"\"\"\n",
       "    # Create callback for model checkpoint\n",
       "    from src.training import model_checkpoint, early_stopping\n",
       "    \n",
       "    checkpoint_path = os.path.join(config['model_dir'], 'bilstm-unet_best.pth')\n",
       "    callbacks = [\n",
       "        model_checkpoint(checkpoint_path, monitor='val_loss', save_best_only=True),\n",
       "        early_stopping(patience=5, monitor='val_loss')\n",
       "    ]\n",
       "    \n",
       "    # Train the model\n",
       "    print(f\"Starting training for {config['epochs']} epochs...\")\n",
       "    try:\n",
       "        history = trainer.fit(\n",
       "            train_loader,\n",
       "            val_loader=val_loader,\n",
       "            epochs=config['epochs'],\n",
       "            callbacks=callbacks\n",
       "        )\n",
       "    except KeyboardInterrupt:\n",
       "        print(\"Training interrupted by user\")\n",
       "    \n",
       "    # Save the final model\n",
       "    model_path = os.path.join(config['model_dir'], 'bilstm-unet_final.pth')\n",
       "    trainer.save_model(model_path)\n",
       "    \n",
       "    return trainer\n",
       "\n",
       "# Train the model (uncomment to run training)\n",
       "# trainer = train_model(trainer, train_loader, val_loader, config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Plot Training History\n",
       "\n",
       "After training, plot the loss and metrics over epochs."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot training history (run after training)\n",
       "def plot_history(trainer, config):\n",
       "    \"\"\"Plot training history\"\"\"\n",
       "    history_path = os.path.join(config['visualize_dir'], 'bilstm-unet_history.png')\n",
       "    trainer.plot_history(save_path=history_path)\n",
       "    \n",
       "# If you've trained the model, uncomment to plot the history\n",
       "# plot_history(trainer, config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Load Best Model and Evaluate\n",
       "\n",
       "Load the best model from checkpoint and evaluate on the test set."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def load_best_model(config):\n",
       "    \"\"\"Load the best model from checkpoint\"\"\"\n",
       "    # Initialize model\n",
       "    channels_per_step = 15 // config['time_steps']\n",
       "    model = BidirectionalLSTMUNet(input_shape=(config['time_steps'], 32, 32, channels_per_step), \n",
       "                                lstm_units=16, unet_filters=16)\n",
       "    \n",
       "    trainer = LSTMUNetTrainer(model, device=device)\n",
       "    trainer.compile(optimizer='adam', learning_rate=config['learning_rate'], criterion='mse')\n",
       "    \n",
       "    # Load checkpoint\n",
       "    checkpoint_path = os.path.join(config['model_dir'], 'bilstm-unet_best.pth')\n",
       "    if os.path.exists(checkpoint_path):\n",
       "        print(f\"Loading model from {checkpoint_path}\")\n",
       "        trainer.load_model(checkpoint_path)\n",
       "    else:\n",
       "        print(f\"Checkpoint not found at {checkpoint_path}. Using untrained model.\")\n",
       "    \n",
       "    return model, trainer\n",
       "\n",
       "# Load the best model\n",
       "# Uncomment after training or if you have a saved model\n",
       "# best_model, best_trainer = load_best_model(config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Evaluate on Test Set"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def evaluate_model(trainer, test_loader, config):\n",
       "    \"\"\"Evaluate the model on the test set\"\"\"\n",
       "    print(\"Evaluating model on test set...\")\n",
       "    test_loss, test_metrics = trainer.evaluate(test_loader)\n",
       "    print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_metrics['mae']:.4f}\")\n",
       "    \n",
       "    # Visualize some predictions\n",
       "    vis_path = os.path.join(config['visualize_dir'], 'bilstm-unet_predictions.png')\n",
       "    \n",
       "    # Custom visualization for Bidirectional LSTM-UNet (adapting for temporal data)\n",
       "    model = trainer.model\n",
       "    model.eval()\n",
       "    device = get_device()\n",
       "    model.to(device)\n",
       "    \n",
       "    # Get samples\n",
       "    samples = []\n",
       "    with torch.no_grad():\n",
       "        for batch_idx, (x, y) in enumerate(test_loader):\n",
       "            if batch_idx >= 5:  # Get 5 samples\n",
       "                break\n",
       "            \n",
       "            x = x.to(device)\n",
       "            pred = model(x)\n",
       "            samples.append((x.cpu().numpy(), y.cpu().numpy(), pred.cpu().numpy()))\n",
       "    \n",
       "    # Plot\n",
       "    fig, axes = plt.subplots(len(samples), 3, figsize=(15, 5*len(samples)))\n",
       "    \n",
       "    for i, (x, y, pred) in enumerate(samples):\n",
       "        # Input (show last time step)\n",
       "        axes[i, 0].imshow(x[0, -1])\n",
       "        axes[i, 0].set_title('Input (Last Time Step)')\n",
       "        axes[i, 0].axis('off')\n",
       "        \n",
       "        # Ground truth\n",
       "        axes[i, 1].imshow(np.transpose(y[0], (1, 2, 0)))\n",
       "        axes[i, 1].set_title('Ground Truth')\n",
       "        axes[i, 1].axis('off')\n",
       "        \n",
       "        # Prediction\n",
       "        axes[i, 2].imshow(np.transpose(pred[0], (1, 2, 0)))\n",
       "        axes[i, 2].set_title('Prediction')\n",
       "        axes[i, 2].axis('off')\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    plt.savefig(vis_path)\n",
       "    plt.show()\n",
       "    \n",
       "    return test_loss, test_metrics\n",
       "\n",
       "# Evaluate on test set\n",
       "# Uncomment after loading a trained model\n",
       "# test_loss, test_metrics = evaluate_model(best_trainer, test_loader, config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 10. Compare with Standard LSTM-UNet\n",
       "\n",
       "Compare the performance of the Bidirectional LSTM-UNet with the standard LSTM-UNet model to see if bidirectional processing improves performance."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def compare_with_lstm_unet(config):\n",
       "    \"\"\"Compare Bidirectional LSTM-UNet with standard LSTM-UNet\"\"\"\n",
       "    from src.models.lstm_unet import LSTMUNet\n",
       "    \n",
       "    # Load the best models if available\n",
       "    channels_per_step = 15 // config['time_steps']\n",
       "    \n",
       "    # Initialize bidirectional model\n",
       "    bilstm_model = BidirectionalLSTMUNet(input_shape=(config['time_steps'], 32, 32, channels_per_step), \n",
       "                                        lstm_units=16, unet_filters=16)\n",
       "    bilstm_trainer = LSTMUNetTrainer(bilstm_model, device=device)\n",
       "    bilstm_trainer.compile(optimizer='adam', learning_rate=config['learning_rate'], criterion='mse')\n",
       "    \n",
       "    # Initialize standard LSTM model\n",
       "    lstm_model = LSTMUNet(input_shape=(config['time_steps'], 32, 32, channels_per_step), \n",
       "                        lstm_units=16, unet_filters=16)\n",
       "    lstm_trainer = LSTMUNetTrainer(lstm_model, device=device)\n",
       "    lstm_trainer.compile(optimizer='adam', learning_rate=config['learning_rate'], criterion='mse')\n",
       "    \n",
       "    # Load checkpoints if available\n",
       "    bilstm_path = os.path.join(config['model_dir'], 'bilstm-unet_best.pth')\n",
       "    lstm_path = os.path.join(config['model_dir'], 'lstm-unet_best.pth')\n",
       "    \n",
       "    models_loaded = True\n",
       "    \n",
       "    if os.path.exists(bilstm_path):\n",
       "        print(f\"Loading Bidirectional LSTM-UNet from {bilstm_path}\")\n",
       "        bilstm_trainer.load_model(bilstm_path)\n",
       "    else:\n",
       "        print(f\"Bidirectional LSTM-UNet checkpoint not found. Using untrained model.\")\n",
       "        models_loaded = False\n",
       "        \n",
       "    if os.path.exists(lstm_path):\n",
       "        print(f\"Loading LSTM-UNet from {lstm_path}\")\n",
       "        lstm_trainer.load_model(lstm_path)\n",
       "    else:\n",
       "        print(f\"LSTM-UNet checkpoint not found. Using untrained model.\")\n",
       "        models_loaded = False\n",
       "    \n",
       "    if not models_loaded:\n",
       "        print(\"Cannot compare models without trained checkpoints.\")\n",
       "        return\n",
       "    \n",
       "    # Create test data loaders\n",
       "    _, _, test_loader = create_data_loaders(X_data, y_data, config)\n",
       "    \n",
       "    # Evaluate both models\n",
       "    print(\"\\nEvaluating Bidirectional LSTM-UNet:\")\n",
       "    bilstm_loss, bilstm_metrics = bilstm_trainer.evaluate(test_loader)\n",
       "    \n",
       "    print(\"\\nEvaluating standard LSTM-UNet:\")\n",
       "    lstm_loss, lstm_metrics = lstm_trainer.evaluate(test_loader)\n",
       "    \n",
       "    # Compare results\n",
       "    print(\"\\nComparison:\")\n",
       "    print(f\"Bidirectional LSTM-UNet - Loss: {bilstm_loss:.4f}, MAE: {bilstm_metrics['mae']:.4f}\")\n",
       "    print(f\"Standard LSTM-UNet - Loss: {lstm_loss:.4f}, MAE: {lstm_metrics['mae']:.4f}\")\n",
       "    \n",
       "    # Plot comparison\n",
       "    plt.figure(figsize=(10, 6))\n",
       "    metrics = ['loss', 'mae']\n",
       "    values = [\n",
       "        [bilstm_loss, bilstm_metrics['mae']],\n",
       "        [lstm_loss, lstm_metrics['mae']]\n",
       "    ]\n",
       "    \n",
       "    x = np.arange(len(metrics))\n",
       "    width = 0.35\n",
       "    \n",
       "    plt.bar(x - width/2, values[0], width, label='Bidirectional LSTM-UNet')\n",
       "    plt.bar(x + width/2, values[1], width, label='Standard LSTM-UNet')\n",
       "    \n",
       "    plt.xlabel('Metrics')\n",
       "    plt.ylabel('Values')\n",
       "    plt.title('Model Comparison')\n",
       "    plt.xticks(x, metrics)\n",
       "    plt.legend()\n",
       "    \n",
       "    plt.savefig(os.path.join(config['visualize_dir'], 'bilstm_vs_lstm_comparison.png'))\n",
       "    plt.show()\n",
       "    \n",
       "    return bilstm_metrics, lstm_metrics\n",
       "\n",
       "# Compare with standard LSTM-UNet\n",
       "# Uncomment to run comparison if you have trained models\n",
       "# bilstm_metrics, lstm_metrics = compare_with_lstm_unet(config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 11. Analyze Bidirectional Processing\n",
       "\n",
       "Analyze how the bidirectional processing affects predictions compared to unidirectional."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def analyze_bidirectional_effect(config):\n",
       "    \"\"\"Analyze how bidirectional processing affects predictions\"\"\"\n",
       "    from src.models.lstm_unet import LSTMUNet\n",
       "    \n",
       "    # Load data\n",
       "    X_data, y_data = load_data(config)\n",
       "    \n",
       "    # Create temporal dataset\n",
       "    channels_per_step = 15 // config['time_steps']\n",
       "    \n",
       "    # Create datasets with original and reversed temporal order\n",
       "    original_dataset = TemporalUrbanCropDataset(X_data[:5], y_data[:5], time_steps=config['time_steps'])\n",
       "    \n",
       "    # Initialize models\n",
       "    bilstm_model = BidirectionalLSTMUNet(input_shape=(config['time_steps'], 32, 32, channels_per_step), \n",
       "                                        lstm_units=16, unet_filters=16)\n",
       "    lstm_model = LSTMUNet(input_shape=(config['time_steps'], 32, 32, channels_per_step), \n",
       "                        lstm_units=16, unet_filters=16)\n",
       "    \n",
       "    # Load pre-trained models if available\n",
       "    bilstm_path = os.path.join(config['model_dir'], 'bilstm-unet_best.pth')\n",
       "    lstm_path = os.path.join(config['model_dir'], 'lstm-unet_best.pth')\n",
       "    \n",
       "    device = get_device()\n",
       "    bilstm_model.to(device)\n",
       "    lstm_model.to(device)\n",
       "    \n",
       "    if os.path.exists(bilstm_path) and os.path.exists(lstm_path):\n",
       "        bilstm_checkpoint = torch.load(bilstm_path, map_location=device)\n",
       "        lstm_checkpoint = torch.load(lstm_path, map_location=device)\n",
       "        \n",
       "        bilstm_model.load_state_dict(bilstm_checkpoint['model_state_dict'])\n",
       "        lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])\n",
       "        print(\"Pre-trained models loaded successfully.\")\n",
       "    else:\n",
       "        print(\"Pre-trained models not found. Using untrained models for demonstration.\")\n",
       "    \n",
       "    # Make predictions\n",
       "    bilstm_model.eval()\n",
       "    lstm_model.eval()\n",
       "    \n",
       "    results = []\n",
       "    \n",
       "    with torch.no_grad():\n",
       "        for i in range(len(original_dataset)):\n",
       "            x, y = original_dataset[i]\n",
       "            x = x.unsqueeze(0).to(device)  # Add batch dimension\n",
       "            \n",
       "            # Forward predictions\n",
       "            bilstm_pred = bilstm_model(x)\n",
       "            lstm_pred = lstm_model(x)\n",
       "            \n",
       "            # Reverse temporal order for standard LSTM (to simulate backward processing)\n",
       "            x_reversed = x.clone()\n",
       "            x_reversed = torch.flip(x_reversed, [1])  # Reverse time dimension\n",
       "            lstm_reversed_pred = lstm_model(x_reversed)\n",
       "            \n",
       "            results.append({\n",
       "                'x': x.cpu().numpy(),\n",
       "                'y': y.cpu().numpy(),\n",
       "                'bilstm_pred': bilstm_pred.cpu().numpy(),\n",
       "                'lstm_pred': lstm_pred.cpu().numpy(),\n",
       "                'lstm_reversed_pred': lstm_reversed_pred.cpu().numpy()\n",
       "            })\n",
       "    \n",
       "    # Visualize one sample\n",
       "    for idx, result in enumerate(results):\n",
       "        plt.figure(figsize=(15, 10))\n",
       "        \n",
       "        # Ground truth\n",
       "        plt.subplot(2, 2, 1)\n",
       "        plt.imshow(np.transpose(result['y'], (1, 2, 0)))\n",
       "        plt.title('Ground Truth')\n",
       "        plt.axis('off')\n",
       "        \n",
       "        # Bidirectional LSTM-UNet prediction\n",
       "        plt.subplot(2, 2, 2)\n",
       "        plt.imshow(np.transpose(result['bilstm_pred'][0], (1, 2, 0)))\n",
       "        plt.title('Bidirectional LSTM-UNet')\n",
       "        plt.axis('off')\n",
       "        \n",
       "        # Standard LSTM-UNet prediction (forward)\n",
       "        plt.subplot(2, 2, 3)\n",
       "        plt.imshow(np.transpose(result['lstm_pred'][0], (1, 2, 0)))\n",
       "        plt.title('Standard LSTM-UNet (Forward)')\n",
       "        plt.axis('off')\n",
       "        \n",
       "        # Standard LSTM-UNet prediction (reversed)\n",
       "        plt.subplot(2, 2, 4)\n",
       "        plt.imshow(np.transpose(result['lstm_reversed_pred'][0], (1, 2, 0)))\n",
       "        plt.title('Standard LSTM-UNet (Reversed)')\n",
       "        plt.axis('off')\n",
       "        \n",
       "        plt.tight_layout()\n",
       "        plt.savefig(os.path.join(config['visualize_dir'], f'bidirectional_analysis_sample_{idx}.png'))\n",
       "        plt.show()\n",
       "        \n",
       "        # Calculate metrics for comparison\n",
       "        bilstm_mae = np.mean(np.abs(result['bilstm_pred'][0] - result['y']))\n",
       "        lstm_mae = np.mean(np.abs(result['lstm_pred'][0] - result['y']))\n",
       "        lstm_reversed_mae = np.mean(np.abs(result['lstm_reversed_pred'][0] - result['y']))\n",
       "        \n",
       "        print(f\"Sample {idx}:\")\n",
       "        print(f\"Bidirectional LSTM-UNet MAE: {bilstm_mae:.4f}\")\n",
       "        print(f\"Standard LSTM-UNet (Forward) MAE: {lstm_mae:.4f}\")\n",
       "        print(f\"Standard LSTM-UNet (Reversed) MAE: {lstm_reversed_mae:.4f}\")\n",
       "        print()\n",
       "        \n",
       "        if idx >= 2:  # Show only first 3 samples to save space\n",
       "            break\n",
       "\n",
       "# Analyze bidirectional processing\n",
       "# Uncomment to run analysis if you have trained models\n",
       "# analyze_bidirectional_effect(config)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 12. Conclusion\n",
       "\n",
       "This notebook demonstrated how to load, train, evaluate, and use the Bidirectional LSTM-UNet model for spatio-temporal urban crop yield prediction. The Bidirectional LSTM-UNet model enhances the standard LSTM-UNet by processing temporal information in both forward and backward directions, potentially capturing more complex temporal dependencies.\n",
       "\n",
       "For a full training, you would increase the number of epochs and potentially tune hyperparameters for better performance. Comparing with the standard LSTM-UNet model can provide insights into the benefits of bidirectional processing for your specific dataset."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }