{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fsIRtVXhTRF"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torch import autograd\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5-0BXDnhWPx",
        "outputId": "9dadddaa-8afb-4966-c8bf-f7a97c558073"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('torch version:',torch.__version__)\n",
        "print('device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phF9BWUxhnm9",
        "outputId": "4a6e49ba-9ca7-464c-860c-4e5eef783f24"
      },
      "outputs": [],
      "source": [
        "train_data_path = '/content/fashion-mnist_train.csv' # Path of data\n",
        "valid_data_path = '/content/fashion-mnist_test.csv' # Path of data\n",
        "print('Train data path:', train_data_path)\n",
        "print('Valid data path:', valid_data_path)\n",
        "\n",
        "img_size = 28 # Image size\n",
        "batch_size = 64  # Batch size\n",
        "\n",
        "# Model\n",
        "z_size = 100\n",
        "generator_layer_size = [256, 512, 1024]\n",
        "discriminator_layer_size = [1024, 512, 256]\n",
        "\n",
        "# Training\n",
        "epochs = 30  # Train epochs\n",
        "learning_rate = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgnc2hJih3Vt",
        "outputId": "27a0b0c9-a0c4-4f48-dc87-fa942603e062"
      },
      "outputs": [],
      "source": [
        "class_list = ['T-Shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "class_num = len(class_list)\n",
        "\n",
        "class FashionMNIST(Dataset):\n",
        "    def __init__(self, path, img_size, transform=None):\n",
        "        self.transform = transform\n",
        "        fashion_df = pd.read_csv(path)\n",
        "        self.images = fashion_df.iloc[:, 1:].values.astype('uint8').reshape(-1, img_size, img_size)\n",
        "        self.labels = fashion_df.label.values\n",
        "        print('Image size:', self.images.shape)\n",
        "        print('--- Label ---')\n",
        "        print(fashion_df.label.value_counts())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        img = self.images[idx]\n",
        "        img = Image.fromarray(self.images[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "dataset = FashionMNIST(train_data_path, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "0T1V3EsOCVhS",
        "outputId": "5c29515c-1897-40fa-826b-32fbfcd81ef0"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])\n",
        "\n",
        "dataset = FashionMNIST(train_data_path, img_size, transform=transform)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for images, labels in data_loader:\n",
        "    fig, ax = plt.subplots(figsize=(18,10))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(make_grid(images, nrow=16).permute(1,2,0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey97PAWvCl1u"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, generator_layer_size, z_size, img_size, class_num):\n",
        "        super().__init__()\n",
        "\n",
        "        self.z_size = z_size\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.label_emb = nn.Embedding(class_num, class_num)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(self.z_size + class_num, generator_layer_size[0]),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(generator_layer_size[0], generator_layer_size[1]),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(generator_layer_size[1], generator_layer_size[2]),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(generator_layer_size[2], self.img_size * self.img_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "\n",
        "        # Reshape z\n",
        "        z = z.view(-1, self.z_size)\n",
        "\n",
        "        # One-hot vector to embedding vector\n",
        "        c = self.label_emb(labels)\n",
        "\n",
        "        # Concat image & label\n",
        "        x = torch.cat([z, c], 1)\n",
        "\n",
        "        # Generator out\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out.view(-1, self.img_size, self.img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E3paLwCCnk8"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, discriminator_layer_size, img_size, class_num):\n",
        "        super().__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(class_num, class_num)\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(self.img_size * self.img_size + class_num, discriminator_layer_size[0]),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(discriminator_layer_size[0], discriminator_layer_size[1]),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(discriminator_layer_size[1], discriminator_layer_size[2]),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(discriminator_layer_size[2], 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "\n",
        "        # Reshape fake image\n",
        "        x = x.view(-1, self.img_size * self.img_size)\n",
        "\n",
        "        # One-hot vector to embedding vector\n",
        "        c = self.label_emb(labels)\n",
        "\n",
        "        # Concat image & label\n",
        "        x = torch.cat([x, c], 1)\n",
        "\n",
        "        # Discriminator out\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UubR0N3iCvu5"
      },
      "outputs": [],
      "source": [
        "# Define generator\n",
        "generator = Generator(generator_layer_size, z_size, img_size, class_num).to(device)\n",
        "# Define discriminator\n",
        "discriminator = Discriminator(discriminator_layer_size, img_size, class_num).to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizer\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lEak02YCn2u"
      },
      "outputs": [],
      "source": [
        "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
        "\n",
        "    # Init gradient\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "    # Building z\n",
        "    z = Variable(torch.randn(batch_size, z_size)).to(device)\n",
        "\n",
        "    # Building fake labels\n",
        "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, class_num, batch_size))).to(device)\n",
        "\n",
        "    # Generating fake images\n",
        "    fake_images = generator(z, fake_labels)\n",
        "\n",
        "    # Disciminating fake images\n",
        "    validity = discriminator(fake_images, fake_labels)\n",
        "\n",
        "    # Calculating discrimination loss (fake images)\n",
        "    g_loss = criterion(validity, Variable(torch.ones(batch_size)).to(device))\n",
        "\n",
        "    # Backword propagation\n",
        "    g_loss.backward()\n",
        "\n",
        "    #  Optimizing generator\n",
        "    g_optimizer.step()\n",
        "\n",
        "    return g_loss.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giDQkxFTC0GC"
      },
      "outputs": [],
      "source": [
        "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
        "\n",
        "    # Init gradient\n",
        "    d_optimizer.zero_grad()\n",
        "\n",
        "    # Disciminating real images\n",
        "    real_validity = discriminator(real_images, labels)\n",
        "\n",
        "    # Calculating discrimination loss (real images)\n",
        "    real_loss = criterion(real_validity, Variable(torch.ones(batch_size)).to(device))\n",
        "\n",
        "    # Building z\n",
        "    z = Variable(torch.randn(batch_size, z_size)).to(device)\n",
        "\n",
        "    # Building fake labels\n",
        "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, class_num, batch_size))).to(device)\n",
        "\n",
        "    # Generating fake images\n",
        "    fake_images = generator(z, fake_labels)\n",
        "\n",
        "    # Disciminating fake images\n",
        "    fake_validity = discriminator(fake_images, fake_labels)\n",
        "\n",
        "    # Calculating discrimination loss (fake images)\n",
        "    fake_loss = criterion(fake_validity, Variable(torch.zeros(batch_size)).to(device))\n",
        "\n",
        "    # Sum two losses\n",
        "    d_loss = real_loss + fake_loss\n",
        "\n",
        "    # Backword propagation\n",
        "    d_loss.backward()\n",
        "\n",
        "    # Optimizing discriminator\n",
        "    d_optimizer.step()\n",
        "\n",
        "    return d_loss.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dfRDP4iEC11h",
        "outputId": "70558e94-abd8-4aa8-de4d-4b00f5cfd77e"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    print('Starting epoch {}...'.format(epoch+1))\n",
        "\n",
        "    for i, (images, labels) in enumerate(data_loader):\n",
        "\n",
        "        # Train data\n",
        "        real_images = Variable(images).to(device)\n",
        "        labels = Variable(labels).to(device)\n",
        "\n",
        "        # Set generator train\n",
        "        generator.train()\n",
        "\n",
        "        # Train discriminator\n",
        "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
        "                                          generator, d_optimizer, criterion,\n",
        "                                          real_images, labels)\n",
        "\n",
        "        # Train generator\n",
        "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
        "\n",
        "    # Set generator eval\n",
        "    generator.eval()\n",
        "\n",
        "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))\n",
        "\n",
        "    # Building z\n",
        "    z = Variable(torch.randn(class_num-1, z_size)).to(device)\n",
        "\n",
        "    # Labels 0 ~ 8\n",
        "    labels = Variable(torch.LongTensor(np.arange(class_num-1))).to(device)\n",
        "\n",
        "    # Generating images\n",
        "    sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
        "\n",
        "    # Show images\n",
        "    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
        "    plt.imshow(grid)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ciNOOGQK64f0",
        "outputId": "ec3e63b2-6498-4735-981f-7ab0fcd03f08"
      },
      "outputs": [],
      "source": [
        "# Building z\n",
        "z = Variable(torch.randn(z_size, z_size)).to(device)\n",
        "\n",
        "# Labels 0 ~ 9\n",
        "labels = Variable(torch.LongTensor([i for _ in range(class_num) for i in range(class_num)])).to(device)\n",
        "\n",
        "# Generating images\n",
        "sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
        "\n",
        "# Show images\n",
        "grid = make_grid(sample_images, nrow=class_num, normalize=True).permute(1,2,0).numpy()\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "ax.imshow(grid)\n",
        "_ = plt.yticks([])\n",
        "_ = plt.xticks(np.arange(15, 300, 30), class_list, rotation=45, fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "63b0zQPzJeTO",
        "outputId": "ffa4dba7-fc7c-47ee-f6f1-9cbf7de3a142"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "load_path = '/content/drive/My Drive/Urbanization Project/arrays of graphs/agriculture_array.npy'\n",
        "loaded_agr_array = np.load(load_path, allow_pickle = True)\n",
        "loaded_agr_array[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97BnM5zGJZW5",
        "outputId": "e4392514-65a5-46de-c4e5-b9e22da11cb1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWeFGRci7hex",
        "outputId": "9a2229f0-4bb4-4d15-c5af-c5f7416eb94c"
      },
      "outputs": [],
      "source": [
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "path = '/content/drive/My Drive/Urbanization Project/Graphs/Crop Yields/agr_1992.png'\n",
        "image = Image.open(path).convert('RGB')\n",
        "\n",
        "new_image = image.resize((300, 300))\n",
        "\n",
        "new_image = np.asarray(new_image)\n",
        "\n",
        "new_image.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
