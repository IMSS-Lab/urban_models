{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEYVeDIYggkz",
        "outputId": "a86c981a-0b92-47ad-a0be-f3c8c68ca87f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#/content/drive/MyDrive/combined_precipitation_array.npy\n",
        "#/content/drive/MyDrive/combined_urbanization_images.npy\n",
        "#/content/drive/MyDrive/crop_yield_images/combined_crop_yield_images.npy\n",
        "\n",
        "precipitation_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/combined_precipitation_array.npy')\n",
        "urbanization_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/combined_urbanization_images.npy')\n",
        "crop_yield_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/combined_crop_yield_images.npy')\n",
        "temperature_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/temperature_final_array.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asGSDWLLjeF1",
        "outputId": "8104bdf4-860c-4fbe-d227-754e7d1627f0"
      },
      "outputs": [],
      "source": [
        "print(precipitation_array.shape)\n",
        "print(urbanization_array.shape)\n",
        "print(crop_yield_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "VpGON5JjKXTO",
        "outputId": "fdb4fd43-a182-4e8c-aea0-1674c4b2f262"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, LSTM, Reshape, TimeDistributed, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "\n",
        "# Set mixed precision\n",
        "set_global_policy('mixed_float16')\n",
        "\n",
        "# Ensure TensorFlow is using GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU\")\n",
        "\n",
        "def build_smaller_hybrid_model(input_shape=(23, 32, 32, 3), lstm_units=16, unet_filters=16):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Temporal processing with LSTM\n",
        "    x = TimeDistributed(Flatten())(inputs)\n",
        "    x = LSTM(lstm_units, return_sequences=False)(x)\n",
        "    x = Dense(32 * 32 * lstm_units)(x)\n",
        "    x = Reshape((32, 32, lstm_units))(x)\n",
        "\n",
        "    # Simplified U-Net\n",
        "    conv1 = Conv2D(unet_filters, 3, activation='relu', padding='same')(x)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(unet_filters*2, 3, activation='relu', padding='same')(pool1)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2, 2))(conv2)\n",
        "    up3 = concatenate([up3, conv1])\n",
        "    conv3 = Conv2D(unet_filters, 3, activation='relu', padding='same')(up3)\n",
        "\n",
        "    outputs = Conv2D(3, 1, activation='linear')(conv3)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def resize_image(img, new_size):\n",
        "    return tf.image.resize(img, new_size).numpy()\n",
        "\n",
        "def preprocess_data(precipitation_array, urbanization_array, crop_yield_array, temperature_array, new_size=(32, 32)):\n",
        "    def resize_array_images(arr):\n",
        "        shape = arr.shape\n",
        "        resized = np.zeros((shape[0], shape[1], shape[2], new_size[0], new_size[1], 3))\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                for k in range(shape[2]):\n",
        "                    resized[i,j,k] = resize_image(arr[i,j,k], new_size)\n",
        "        return resized\n",
        "\n",
        "    precipitation_array = resize_array_images(precipitation_array)\n",
        "    urbanization_array = resize_array_images(urbanization_array)\n",
        "    crop_yield_array = resize_array_images(crop_yield_array)\n",
        "    temperature_array = resize_array_images(temperature_array)\n",
        "\n",
        "    return precipitation_array, urbanization_array, crop_yield_array, temperature_array\n",
        "\n",
        "def data_generator(precipitation_array, urbanization_array, crop_yield_array, temperature_array, batch_size=32, split='train'):\n",
        "    years, lat_partitions, lon_partitions, height, width, channels = precipitation_array.shape\n",
        "    train_years = int(0.8 * years)\n",
        "\n",
        "    while True:\n",
        "        if split == 'train':\n",
        "            year_range = range(5, train_years)\n",
        "        else:\n",
        "            year_range = range(train_years, years)\n",
        "\n",
        "        for year in year_range:\n",
        "            for lat in range(0, lat_partitions, batch_size):\n",
        "                for lon in range(0, lon_partitions, batch_size):\n",
        "                    X_batch = []\n",
        "                    y_batch = []\n",
        "\n",
        "                    for i in range(lat, min(lat + batch_size, lat_partitions)):\n",
        "                        for j in range(lon, min(lon + batch_size, lon_partitions)):\n",
        "                            x_sample = []\n",
        "\n",
        "                            # Last 5 years of all data\n",
        "                            for past_year in range(year-5, year):\n",
        "                                x_sample.append(crop_yield_array[past_year, i, j])\n",
        "                                x_sample.append(temperature_array[past_year, i, j])\n",
        "                                x_sample.append(precipitation_array[past_year, i, j])\n",
        "                                x_sample.append(urbanization_array[past_year, i, j])\n",
        "\n",
        "                            # Current year's temp, precip, urban\n",
        "                            x_sample.append(temperature_array[year, i, j])\n",
        "                            x_sample.append(precipitation_array[year, i, j])\n",
        "                            x_sample.append(urbanization_array[year, i, j])\n",
        "\n",
        "                            X_batch.append(np.array(x_sample))\n",
        "                            y_batch.append(crop_yield_array[year, i, j])\n",
        "\n",
        "                    if len(X_batch) == batch_size:\n",
        "                        yield np.array(X_batch), np.array(y_batch)\n",
        "                        X_batch = []\n",
        "                        y_batch = []\n",
        "\n",
        "        # Yield any remaining samples\n",
        "        if X_batch:\n",
        "            yield np.array(X_batch), np.array(y_batch)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    precipitation_array = np.load('/content/drive/MyDrive/combined_precipitation_array.npy')\n",
        "    urbanization_array = np.load('/content/drive/MyDrive/combined_urbanization_images.npy')\n",
        "    crop_yield_array = np.load('/content/drive/MyDrive/crop_yield_images/combined_crop_yield_images.npy')\n",
        "    temperature_array = np.load('/content/drive/MyDrive/Copy of temperature_final_array.npy')\n",
        "\n",
        "    # Preprocess data\n",
        "    new_size = (32, 32)\n",
        "    precipitation_array, urbanization_array, crop_yield_array, temperature_array = preprocess_data(\n",
        "        precipitation_array, urbanization_array, crop_yield_array, temperature_array, new_size)\n",
        "\n",
        "    # Create the generators\n",
        "    batch_size = 32  # Set batch size\n",
        "    train_gen = data_generator(precipitation_array, urbanization_array, crop_yield_array, temperature_array, batch_size=batch_size, split='train')\n",
        "    val_gen = data_generator(precipitation_array, urbanization_array, crop_yield_array, temperature_array, batch_size=batch_size, split='val')\n",
        "\n",
        "    # Build and compile the model\n",
        "    input_shape = (23, 32, 32, 3)  # 23 time steps, 32x32 image, 3 channels\n",
        "    model = build_smaller_hybrid_model(input_shape)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)  # Increased learning rate\n",
        "    model.compile(optimizer=optimizer,\n",
        "              loss='mse',\n",
        "              metrics=['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
        "    # Calculate steps per epoch\n",
        "    years, lat_partitions, lon_partitions, _, _, _ = precipitation_array.shape\n",
        "    train_samples = (int(0.8 * years) - 5) * lat_partitions * lon_partitions\n",
        "    steps_per_epoch = train_samples // batch_size\n",
        "    validation_steps = (years - int(0.8 * years)) * lat_partitions * lon_partitions // batch_size\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs=9,\n",
        "                        validation_data=val_gen, validation_steps=validation_steps)\n",
        "\n",
        "    # Print final metrics\n",
        "    print(f\"Final Training MAE: {history.history['mae'][-1]:.4f}\")\n",
        "    print(f\"Final Validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save('smaller_hybrid_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOg9SZbi0Eby",
        "outputId": "cc9a7638-c57c-4924-cd4f-aff81f537020"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Bidirectional, Conv2D, MaxPooling2D, UpSampling2D, concatenate, LSTM, Reshape, TimeDistributed, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "\n",
        "# Set mixed precision\n",
        "set_global_policy('mixed_float16')\n",
        "\n",
        "# Ensure TensorFlow is using GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU\")\n",
        "\n",
        "def build_smaller_hybrid_model(input_shape=(23, 32, 32, 3), lstm_units=16, unet_filters=16):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Temporal processing with LSTM\n",
        "    x = TimeDistributed(Flatten())(inputs)\n",
        "    x = Bidirectional(LSTM(lstm_units, return_sequences=False))(x)\n",
        "    x = Dense(32 * 32 * lstm_units)(x)\n",
        "    x = Reshape((32, 32, lstm_units))(x)\n",
        "\n",
        "    # Simplified U-Net\n",
        "    conv1 = Conv2D(unet_filters, 3, activation='relu', padding='same')(x)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(unet_filters*2, 3, activation='relu', padding='same')(pool1)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2, 2))(conv2)\n",
        "    up3 = concatenate([up3, conv1])\n",
        "    conv3 = Conv2D(unet_filters, 3, activation='relu', padding='same')(up3)\n",
        "\n",
        "    outputs = Conv2D(3, 1, activation='linear')(conv3)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "    # Simplified U-Net\n",
        "    conv1 = Conv2D(unet_filters, 3, activation='relu', padding='same')(x)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(unet_filters*2, 3, activation='relu', padding='same')(pool1)\n",
        "\n",
        "    up3 = UpSampling2D(size=(2, 2))(conv2)\n",
        "    up3 = concatenate([up3, conv1])\n",
        "    conv3 = Conv2D(unet_filters, 3, activation='relu', padding='same')(up3)\n",
        "\n",
        "    outputs = Conv2D(3, 1, activation='linear')(conv3)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def resize_image(img, new_size):\n",
        "    return tf.image.resize(img, new_size).numpy()\n",
        "\n",
        "def preprocess_data(precipitation_array, urbanization_array, crop_yield_array, temperature_array, new_size=(32, 32)):\n",
        "    def resize_array_images(arr):\n",
        "        shape = arr.shape\n",
        "        resized = np.zeros((shape[0], shape[1], shape[2], new_size[0], new_size[1], 3))\n",
        "        for i in range(shape[0]):\n",
        "            for j in range(shape[1]):\n",
        "                for k in range(shape[2]):\n",
        "                    resized[i,j,k] = resize_image(arr[i,j,k], new_size)\n",
        "        return resized\n",
        "\n",
        "    precipitation_array = resize_array_images(precipitation_array)\n",
        "    urbanization_array = resize_array_images(urbanization_array)\n",
        "    crop_yield_array = resize_array_images(crop_yield_array)\n",
        "    temperature_array = resize_array_images(temperature_array)\n",
        "\n",
        "    return precipitation_array, urbanization_array, crop_yield_array, temperature_array\n",
        "\n",
        "def data_generator(precipitation_array, urbanization_array, crop_yield_array, temperature_array, batch_size=32, split='train'):\n",
        "    years, lat_partitions, lon_partitions, height, width, channels = precipitation_array.shape\n",
        "    train_years = int(0.8 * years)\n",
        "\n",
        "    while True:\n",
        "        if split == 'train':\n",
        "            year_range = range(5, train_years)\n",
        "        else:\n",
        "            year_range = range(train_years, years)\n",
        "\n",
        "        for year in year_range:\n",
        "            for lat in range(0, lat_partitions, batch_size):\n",
        "                for lon in range(0, lon_partitions, batch_size):\n",
        "                    X_batch = []\n",
        "                    y_batch = []\n",
        "\n",
        "                    for i in range(lat, min(lat + batch_size, lat_partitions)):\n",
        "                        for j in range(lon, min(lon + batch_size, lon_partitions)):\n",
        "                            x_sample = []\n",
        "\n",
        "                            # Last 5 years of all data\n",
        "                            for past_year in range(year-5, year):\n",
        "                                x_sample.append(crop_yield_array[past_year, i, j])\n",
        "                                x_sample.append(temperature_array[past_year, i, j])\n",
        "                                x_sample.append(precipitation_array[past_year, i, j])\n",
        "                                x_sample.append(urbanization_array[past_year, i, j])\n",
        "\n",
        "                            # Current year's temp, precip, urban\n",
        "                            x_sample.append(temperature_array[year, i, j])\n",
        "                            x_sample.append(precipitation_array[year, i, j])\n",
        "                            x_sample.append(urbanization_array[year, i, j])\n",
        "\n",
        "                            X_batch.append(np.array(x_sample))\n",
        "                            y_batch.append(crop_yield_array[year, i, j])\n",
        "\n",
        "                    if len(X_batch) == batch_size:\n",
        "                        yield np.array(X_batch), np.array(y_batch)\n",
        "                        X_batch = []\n",
        "                        y_batch = []\n",
        "\n",
        "        # Yield any remaining samples\n",
        "        if X_batch:\n",
        "            yield np.array(X_batch), np.array(y_batch)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    precipitation_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/combined_precipitation_array.npy')\n",
        "    urbanization_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/combined_urbanization_images.npy')\n",
        "    crop_yield_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/combined_crop_yield_images.npy')\n",
        "    temperature_array = np.load('/content/drive/MyDrive/Urbanization Project/arrays of graphs/temperature_final_array.npy')\n",
        "\n",
        "    # Preprocess data\n",
        "    new_size = (32, 32)\n",
        "    precipitation_array, urbanization_array, crop_yield_array, temperature_array = preprocess_data(\n",
        "        precipitation_array, urbanization_array, crop_yield_array, temperature_array, new_size)\n",
        "\n",
        "    # Create the generators\n",
        "    batch_size = 32  # Set batch size\n",
        "    train_gen = data_generator(precipitation_array, urbanization_array, crop_yield_array, temperature_array, batch_size=batch_size, split='train')\n",
        "    val_gen = data_generator(precipitation_array, urbanization_array, crop_yield_array, temperature_array, batch_size=batch_size, split='val')\n",
        "\n",
        "    # Build and compile the model\n",
        "    input_shape = (23, 32, 32, 3)  # 23 time steps, 32x32 image, 3 channels\n",
        "    model = build_smaller_hybrid_model(input_shape)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)  # Increased learning rate\n",
        "    model.compile(optimizer=optimizer,\n",
        "              loss='mse',\n",
        "              metrics=['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
        "    # Calculate steps per epoch\n",
        "    years, lat_partitions, lon_partitions, _, _, _ = precipitation_array.shape\n",
        "    train_samples = (int(0.8 * years) - 5) * lat_partitions * lon_partitions\n",
        "    steps_per_epoch = train_samples // batch_size\n",
        "    validation_steps = (years - int(0.8 * years)) * lat_partitions * lon_partitions // batch_size\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs=9,\n",
        "                        validation_data=val_gen, validation_steps=validation_steps)\n",
        "\n",
        "    # Print final metrics\n",
        "    print(f\"Final Training MAE: {history.history['mae'][-1]:.4f}\")\n",
        "    print(f\"Final Validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save('UNet_hybrid_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcj2-jBbd8TQ",
        "outputId": "7f309a08-3e99-4f81-f7a2-87dd46e7237b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZydrmcDSPHU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
